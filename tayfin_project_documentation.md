# Tayfin Project Comprehensive Documentation

## Project Overview

Tayfin is a local-first suite of services for ingesting market data, screening, analysis, and a user-facing app. This monorepo contains multiple bounded contexts that communicate strictly via HTTP APIs, with no shared domain code or cross-schema database reads.

### Key Philosophy
- **Structure-driven**: Strict bounded contexts with no shared DTOs/enums/models across contexts.
- **API-only communication**: Cross-context data access happens via HTTP APIs only, not direct database queries.
- **Ephemeral jobs**: Jobs are run-once tasks that start, perform work, audit execution, and exit.
- **Local-first**: Designed to run via Docker Compose or partial local execution.

### Architecture Contexts
- `tayfin-ingestor/`: Ingestion jobs and read-only API for market data
- `tayfin-screener/`: Screening jobs and read-only API for screening results  
- `tayfin-analyzer/`: Analysis jobs, report creation, and read-only API
- `tayfin-app/`: React UI and Backend-for-Frontend (BFF)

## Tech Stack

### Python Baseline
- **Python 3.11** for all Python applications
- Each app has its own `requirements.txt`
- Typer for CLI tooling
- Flask for APIs
- SQLAlchemy Core for database access
- httpx for HTTP clients
- Standard `logging` module for operational logs
- ruff for linting and formatting

### Database
- **PostgreSQL** with one schema per context
- Flyway for migrations (one history table per schema)
- Schema naming: `tayfin_ingestor`, `tayfin_screener`, `tayfin_analyzer`

### Configuration
- Precedence: CLI flags > environment variables > YAML config > code defaults
- `.env` files supported for local development
- YAML configs for jobs with optional date ranges

## Architecture Rules (Non-Negotiable)

### Bounded Contexts
- Each context owns its domain concepts, DTOs, API models, database schema, and business logic
- **NO shared domain code** across contexts - duplication preferred over coupling
- Example: Both ingestor and screener may define "Stock" independently

### Cross-Context Communication
- **API-only rule**: All cross-context data access via HTTP APIs
- Services/jobs **MUST NOT** query another context's schema
- UI calls only BFF; BFF may call any context API

### Jobs
- **Ephemeral and run-once**: Jobs start, perform single logical task, audit, and exit
- **Orchestrate, don't implement**: Jobs call providers/repositories; no SQL/HTTP logic in jobs
- **Best-effort processing**: Continue on item failure but mark run as FAILED if any item fails
- **Idempotent**: Safe to re-run with deterministic upserts
- **Audited**: Every job run recorded in database with provenance tracking

### Database
- **Schema-per-context**: One Postgres instance, one schema per context
- **Table provenance**: All tables include `created_at`, `updated_at`, `created_by_job_run_id`
- **No cross-schema reads**: Contexts don't read other schemas
- **Flyway**: Each context owns its migrations; undo scripts only for destructive changes

### Reporting
- **Created by jobs**: Reports generated by jobs, served read-only by APIs
- **Canonical format**: JSON (required), HTML snapshot (optional)
- **Derived exports**: CSV/PDF/XLSX generated from canonical JSON
- **Immutable**: New computations create new reports; overwrites only via explicit maintenance

## Glossary

### Bounded Context
Logical boundary containing consistent domain models, rules, terminology, and data ownership. In Tayfin: ingestor, screener, analyzer, app.

### Job
Ephemeral, run-once process performing a single logical task, processing items, auditing execution, and exiting.

### Job Run
Single execution instance with unique ID, timestamps, status (SUCCESS/FAILED), and audit data. All writes attributed to a job run.

### Item
Smallest processing unit in a job (e.g., one stock symbol). Jobs atomic at item level.

### Idempotency
Re-running same job doesn't corrupt data or create duplicates.

### Upsert
Insert if not exists, update if exists. Ensures idempotency.

### Provenance
Tracking data origin: `created_at`, `updated_at`, `created_by_job_run_id`, `updated_by_job_run_id`.

### Report
Structured artifact from domain data. Created by jobs, canonical JSON format.

### BFF (Backend-for-Frontend)
Backend service serving UI, aggregating data from multiple APIs.

### Local-first
Prioritizes local development and Docker Compose execution.

## Phase 0 Decisions (Frozen)

### Monorepo Structure
- Single GitHub repository
- Contexts at repo root level
- Each context contains jobs + API (app context: UI + BFF)
- Independent CI/CD per app

### Job Semantics
- Ephemeral run-once tasks
- Orchestrate collaborators (repositories/providers)
- Best-effort item processing with run failure on any item failure
- Idempotent via deterministic upserts with unique constraints
- Audited with job run records

### Database & Migrations
- Single Postgres instance
- Schema per context, owned by context
- Row provenance tracking
- Flyway runs centrally, history per schema
- Undo scripts only for destructive migrations

### Reporting
- Jobs create reports, APIs serve artifacts
- JSON canonical, HTML optional, exports derived
- Immutable reports, new computations create new records

## Components Deep Dive

### tayfin-ingestor
**Purpose**: Ingest market data into `tayfin_ingestor` schema.

**Jobs**:
- Discovery: Resolve index memberships, upsert instruments and index_memberships
- Fundamentals: Compute daily snapshots using Stockdex/Yahoo

**API Endpoints**:
- `GET /health`
- `GET /fundamentals/latest?symbol=X`
- `GET /fundamentals?symbol=X&from=Y&to=Z`
- `GET /indices/members?index_code=X`
- `GET /indices/by-symbol?symbol=X`

**Database Tables**:
- `instruments`
- `index_memberships` 
- `fundamentals_snapshots` (time-series keyed by `(instrument_id, as_of_date, source)`)

### tayfin-screener
**Purpose**: Run screening jobs and serve results from `tayfin_screener` schema.

**Jobs**: Screening computations (implementation pending)

**API**: Read-only API over screening results (implementation pending)

### tayfin-analyzer
**Purpose**: Run analysis jobs, create reports in `tayfin_analyzer` schema.

**Jobs**: Analysis and report generation (implementation pending)

**API**: Read-only API over analysis and reports (implementation pending)

### tayfin-app
**Purpose**: User-facing application.

**UI**: React application (implementation pending)

**BFF**: Backend-for-Frontend aggregating data from context APIs (implementation pending)

## Data Sources & Libraries

### Stockdex
Python wrapper for multiple financial data sources (Yahoo, Macrotrends, Finviz, Digrin, JustETF).

**Key Methods**:
- `yahoo_api_*`: Reliable but rate-limited (price, statements, ratios, profile)
- `yahoo_web_*`: Fragile web scraping (statements, profile)
- `macrotrends_*`: Partial coverage (statements, ratios)
- `finviz_*`: Ratios and profile data
- `digrin_*`: Partial statements and ratios
- `justetf_*`: ETF holdings and composition

**Usage**: See examples in `docs/examples/stockdex/`

### tradingview-scraper
Programmatic scraping of TradingView resources.

**Capabilities**:
- Ideas: Symbol ideas with title, author, body, timestamps
- Indicators: Technical indicators (RSI, Stoch) by timeframe
- News: Headlines and article metadata
- Screener: Filtered symbol lists with market data
- OHLCV: Symbol overview and time-series data

**Notes**: Subject to rate limits and captchas; requires session cookies for reliability.

**Usage**: See examples in `docs/examples/tradingview_scraper/`

## Local Development Setup

### Prerequisites
- Docker + Docker Compose
- Python 3.11

### Quick Start
1. Copy env: `cp .env.example .env`
2. Start DB: `docker compose --env-file .env -f infra/docker-compose.yml up`
3. DB available on localhost:5432

### Running Components
- **Ingestor API**: `PYTHONPATH=src flask --app tayfin_ingestor_api.app run --host 0.0.0.0 --port 8000`
- **Ingestor Jobs**: `PYTHONPATH=src python -m tayfin_ingestor_jobs jobs list --config config.yml`
- Other components: Follow respective READMEs

### Job Execution Examples
```bash
# List jobs
PYTHONPATH=src python -m tayfin_ingestor_jobs jobs list

# Run discovery
PYTHONPATH=src python -m tayfin_ingestor_jobs jobs run discovery nasdaq-100 --config config/discovery.yml

# Run fundamentals
PYTHONPATH=src python -m tayfin_ingestor_jobs jobs run fundamentals nasdaq-100 --config config/fundamentals.yml
```

## Key Design Patterns

### Repository Pattern
- Database access behind repository interfaces
- Jobs orchestrate repositories; repositories handle persistence
- Example: `FundamentalsRepository` for fundamentals data

### Provider Pattern
- External data source access behind provider interfaces
- Providers handle HTTP calls, parsing, retries
- Example: `StockdexFundamentalsProvider`

### Configuration Hierarchy
- Code defaults → YAML → Environment → CLI
- Jobs support `--config path/to/config.yml`
- Dates optional in YAML (sensible defaults when omitted)

### Audit & Provenance
- Every write tagged with `created_by_job_run_id`
- Job runs have unique IDs, start/end times, status
- Enables traceability and debugging

### Idempotent Upserts
- Unique constraints on natural keys: `(symbol, as_of_date, provider)`
- `ON CONFLICT` clauses for safe re-runs
- No duplicate data on job re-execution

## Development Guidelines

### Code Quality
- Prefer clarity over cleverness
- Keep jobs thin: orchestrate, don't implement heavy logic
- Use explicit interfaces for side effects
- Document decisions when architecture unclear

### Testing
- Validate job idempotency
- Test provider error handling
- Verify API responses match schemas

### Deployment
- Local-first: Docker Compose primary
- Cloud secondary (GCP Cloud Run planned)
- Configuration from env vars, not hardcoded

## Future Phases

### Deferred Items
- Authentication/authorization
- Cloud deployment specifics
- Production hardening/scaling
- Scheduling mechanism
- Exact API endpoint catalog

### Phase 1: Repo Skeleton
- Implement folder layouts
- Basic scaffolding aligned to Phase 0 rules

This documentation provides a complete foundation for understanding the Tayfin project's architecture, components, and development practices. Use it to train Google Gemini on the project's structure and constraints.